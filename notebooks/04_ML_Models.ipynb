{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a46b20b",
   "metadata": {},
   "source": [
    "# Transformer Apply for QQQ ETF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab245b",
   "metadata": {},
   "source": [
    "## 数据预处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55b112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspaces/AI-Trader')\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from models.ml.transformer import TimeSeriesTransformer  # Import the optimized model\n",
    "from features.feature_store import process_features  # Import feature processing pipeline\n",
    "# Data loading and preprocessing\n",
    "def load_and_preprocess_data(file_path, seq_len=30):\n",
    "    \"\"\"\n",
    "    Load and preprocess time series data for model training.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        seq_len (int): Sequence length for time series.\n",
    "\n",
    "    Returns:\n",
    "        X (torch.Tensor): Input sequences.\n",
    "        y (torch.Tensor): Target values.\n",
    "        scaler (MinMaxScaler): Scaler for inverse transformation.\n",
    "    \"\"\"\n",
    "    # Read data\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Select needed columns\n",
    "    data = data[['Date', 'Close']]\n",
    "    \n",
    "    # Convert date format and sort (address pandas warning)\n",
    "    data['Date'] = pd.to_datetime(data['Date'], utc=True)  # Set utc=True to address warning\n",
    "    data = data.sort_values('Date')\n",
    "\n",
    "    # Process features using the feature pipeline\n",
    "    processed_features = process_features(data)\n",
    "    \n",
    "    # Normalize the 'Close' column for sequence creation\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    processed_features['Close'] = scaler.fit_transform(processed_features[['Close']])\n",
    "    \n",
    "    # Create sequences efficiently\n",
    "    close_values = processed_features['Close'].values\n",
    "    X, y = create_sequences(close_values, seq_len)\n",
    "    \n",
    "    return X, y, scaler\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    \"\"\"\n",
    "    Create time series sequences efficiently.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Input data (e.g., features).\n",
    "        seq_len (int): Sequence length.\n",
    "\n",
    "    Returns:\n",
    "        X (torch.Tensor): Input sequences.\n",
    "        y (torch.Tensor): Target values.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(data) - seq_len):\n",
    "        seq = data[i:i + seq_len]\n",
    "        target = data[i + seq_len]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    # Convert to numpy arrays first, then to tensors\n",
    "    sequences_np = np.array(sequences)\n",
    "    targets_np = np.array(targets)\n",
    "    \n",
    "    return torch.from_numpy(sequences_np).float(), torch.from_numpy(targets_np).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13afd9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "def split_data(X, y, train_ratio=0.7, val_ratio=0.15):\n",
    "    \"\"\"Split data into training, validation, and test sets\"\"\"\n",
    "    train_size = int(train_ratio * len(X))\n",
    "    val_size = int(val_ratio * len(X))\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X[:train_size], X[train_size:train_size + val_size]\n",
    "    X_test = X[train_size + val_size:]\n",
    "    y_train, y_val = y[:train_size], y[train_size:train_size + val_size]\n",
    "    y_test = y[train_size + val_size:]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849bc82",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89826793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function\n",
    "def train_model(model, X_train, y_train, X_val, y_val, num_epochs=50, batch_size=32, \n",
    "               patience=10, lr=0.001, weight_decay=1e-5):\n",
    "    \"\"\"Train the model with early stopping and learning rate scheduler\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Move data to device (properly)\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training tracking\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_acc = model.train_epoch(X_train, y_train, optimizer, batch_size=batch_size)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_acc = model.validate(X_val, y_val, batch_size=batch_size)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Record metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.2f}, Val Acc: {val_acc:.2f}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
    "            print(f\"Model saved at epoch {epoch + 1} with validation loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba4377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and visualization\n",
    "def evaluate_model(model, X_test, y_test, batch_size=32):\n",
    "    \"\"\"Evaluate model on test data and visualize results\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_transformer_model.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    # Test evaluation\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_loss, test_acc = model.validate(X_test, y_test, batch_size=batch_size)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Direction Accuracy: {test_acc:.2f}\")\n",
    "    \n",
    "    # Prediction and metrics\n",
    "    predictions = model.predict(X_test)\n",
    "    y_test_np = y_test.cpu().numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_test_np, predictions)\n",
    "    mae = mean_absolute_error(y_test_np, predictions)\n",
    "    direction_accuracy = np.mean(np.sign(predictions) == np.sign(y_test_np))\n",
    "    \n",
    "    print(f\"Detailed evaluation metrics:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"Direction Accuracy: {direction_accuracy:.2f}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_results(train_losses, val_losses, train_accuracies, val_accuracies, \n",
    "                     y_test_np, predictions)\n",
    "    \n",
    "    return mse, mae, direction_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56e978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(train_losses, val_losses, train_accuracies, val_accuracies, \n",
    "                     y_test, predictions):\n",
    "    \"\"\"Visualize training history and predictions\"\"\"\n",
    "    # Plot loss curves\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')  # Use log scale for better loss visualization\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curve')\n",
    "    \n",
    "    # Plot accuracy curves\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(val_accuracies, label='Val Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title('Direction Accuracy Curve')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('transformer_training_curves.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot predictions vs actual values\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(y_test, label='Actual Values', color='blue')\n",
    "    plt.plot(predictions, label='Predicted Values', color='red', linestyle='--')\n",
    "    plt.title('Prediction Results Comparison')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('prediction_results.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3cf9c2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m X, y, scaler = load_and_preprocess_data(file_path, seq_len)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Get feature dimension dynamically\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m feature_dim = \u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Number of features\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Split data\u001b[39;00m\n\u001b[32m     12\u001b[39m X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "   # Load and preprocess data\n",
    "    file_path = '/workspaces/AI-Trader/data/raw/qqq_history.csv'\n",
    "    seq_len = 30\n",
    "    X, y, scaler = load_and_preprocess_data(file_path, seq_len)\n",
    "\n",
    "    # Get feature dimension dynamically\n",
    "    feature_dim = X.shape[2]  # Number of features\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    feature_dim = 1  # Single feature (closing price)\n",
    "    num_heads = 4  # Number of attention heads\n",
    "    hidden_dim = 128  # Hidden dimension\n",
    "    num_layers = 2  # Number of Transformer layers\n",
    "    dropout = 0.2  # Dropout rate\n",
    "    \n",
    "    # Initialize model\n",
    "    model = TimeSeriesTransformer(\n",
    "        feature_dim=feature_dim,\n",
    "        seq_len=seq_len,\n",
    "        num_heads=num_heads,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "        model, X_train, y_train, X_val, y_val\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9673a6",
   "metadata": {},
   "source": [
    "## 预测与策略生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae08862",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m predictions = model.predict(X_test)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 反归一化\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m predictions = scaler.inverse_transform(\u001b[43mpredictions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m().numpy())\n\u001b[32m      6\u001b[39m y_test = scaler.inverse_transform(y_test.numpy().reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 预测\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 反归一化\n",
    "predictions = scaler.inverse_transform(predictions.detach().numpy())\n",
    "y_test = scaler.inverse_transform(y_test.numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c528eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date      Actual   Predicted Signal\n",
      "0  2020-01-27 00:00:00-05:00  211.204575 -288.426941   Sell\n",
      "1  2020-01-28 00:00:00-05:00  214.448685 -288.426941   Sell\n",
      "2  2020-01-29 00:00:00-05:00  214.797256 -288.426941   Sell\n",
      "3  2020-01-30 00:00:00-05:00  215.562286 -288.426941   Sell\n",
      "4  2020-01-31 00:00:00-05:00  212.143906 -288.426941   Sell\n"
     ]
    }
   ],
   "source": [
    "# 生成交易信号：根据预测值生成买入、卖出或持有信号：\n",
    "import numpy as np\n",
    "\n",
    "# 简单策略：如果预测价格高于当前价格，则买入；否则卖出\n",
    "signals = []\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] > y_test[i]:\n",
    "        signals.append('Buy')\n",
    "    else:\n",
    "        signals.append('Sell')\n",
    "\n",
    "# 将信号与日期对应\n",
    "results = pd.DataFrame({\n",
    "    'Date': data['Date'].iloc[train_size + seq_len:].values,\n",
    "    'Actual': y_test.flatten(),\n",
    "    'Predicted': predictions.flatten(),\n",
    "    'Signal': signals\n",
    "})\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3acecb4",
   "metadata": {},
   "source": [
    "## 评估策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8833aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 413447.2500, RMSE: 642.9986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f'MSE: {mse:.4f}, RMSE: {rmse:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
